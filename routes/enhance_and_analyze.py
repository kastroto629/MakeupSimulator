from fastapi import APIRouter, UploadFile, File
from fastapi.responses import JSONResponse
import os
import cv2
import numpy as np
import torch
import base64

from basicsr.utils import img2tensor, tensor2img
from torchvision.transforms.functional import normalize
from basicsr.utils.registry import ARCH_REGISTRY
from basicsr.utils.download_util import load_file_from_url

from facelib.detection import init_detection_model
from facelib.utils.face_restoration_helper import get_largest_face
from facelib.utils.face_utils import align_crop_face_landmarks, paste_face_back
from facelib.utils.color_utils import extract_face_colors

router = APIRouter()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# CodeFormer ëª¨ë¸ ì´ˆê¸°í™”
net = ARCH_REGISTRY.get("CodeFormer")(dim_embd=512, codebook_size=1024, n_head=8, n_layers=9,
                                      connect_list=["32", "64", "128", "256"]).to(device)
ckpt_path = load_file_from_url(
    url="https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth",
    model_dir="weights/CodeFormer", progress=True)
net.load_state_dict(torch.load(ckpt_path)["params_ema"])
net.eval()


@router.post("/enhance_and_analyze")
async def enhance_and_analyze(file: UploadFile = File(...)):
    print(f"ğŸ“¥ ë¶„ì„ ìš”ì²­ ë°›ìŒ: {file.filename}")
    contents = await file.read()
    print(f"ğŸ“¦ íŒŒì¼ í¬ê¸°: {len(contents)} bytes")
    img = cv2.imdecode(np.frombuffer(contents, np.uint8), cv2.IMREAD_COLOR)
    if img is None:
        print("âŒ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨")
        return JSONResponse(status_code=400, content={"error": "ì´ë¯¸ì§€ë¥¼ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."})

    h, w = img.shape[:2]
    scale = max(h / 800, w / 800)
    img_scale = cv2.resize(img, (int(w / scale), int(h / scale)), interpolation=cv2.INTER_LINEAR) if scale > 1 else img

    # ì–¼êµ´ ê²€ì¶œ ë° landmark
    det_net = init_detection_model("retinaface_resnet50", half=False)
    bboxes = det_net.detect_faces(img_scale, 0.97)
    if scale > 1:
        bboxes *= scale
    if len(bboxes) == 0:
        return JSONResponse(status_code=404, content={"error": "ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."})
    bboxes = get_largest_face(bboxes, h, w)[0]
    landmarks = np.array([[bboxes[i], bboxes[i + 1]] for i in range(5, 15, 2)])

    # ì–¼êµ´ ì •ë ¬
    aligned_face, inverse_affine = align_crop_face_landmarks(
        img, landmarks, output_size=512, return_inverse_affine=True)

    # CodeFormer ë³µì›
    face_tensor = img2tensor(aligned_face / 255., bgr2rgb=True, float32=True).unsqueeze(0).to(device)
    normalize(face_tensor, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)
    with torch.no_grad():
        output = net(face_tensor, w=0.7, adain=True)[0]
        restored_face = tensor2img(output, rgb2bgr=True, min_max=(-1, 1)).astype("uint8")

    # ì–¼êµ´ ì›ë³¸ ì´ë¯¸ì§€ì— ë‹¤ì‹œ ë¶™ì´ê¸°
    restored_full_img = paste_face_back(img.astype(np.float32), restored_face, inverse_affine).astype("uint8")

    # ìƒ‰ìƒ ì¶”ì¶œ (ì…ìˆ , í™ì±„, ëˆˆì¹)
    rgb_image = cv2.cvtColor(restored_face, cv2.COLOR_BGR2RGB)
    lip_hex, lip_html, iris_hex, iris_html, brow_hex, brow_html = extract_face_colors(rgb_image)

    # ì´ë¯¸ì§€ base64 ì¸ì½”ë”©
    _, buffer = cv2.imencode(".png", restored_full_img)
    result_image_base64 = base64.b64encode(buffer).decode("utf-8")

    return {
        "restored_image": result_image_base64,
        "lip_hex": lip_hex, "lip_html": lip_html,
        "iris_hex": iris_hex, "iris_html": iris_html,
        "brow_hex": brow_hex, "brow_html": brow_html
    }
